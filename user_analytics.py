from load_data_from_db import load_data_for_analytics
import asyncio
import pandas as pd


async def prepare_aggregate_data_by_period_and_draw_analytic_for_user(period: str = 'week'):
    """Loads user analytics data, enriches it with error statistics, 
    calculates session-based metrics, and returns a merged DataFrame."""

    # üîπ Load data for a specific user (replace this with a loop for all users if needed)
    dfs = await load_data_for_analytics(117649764)

    all_user_sentences = dfs["sentences"].copy()
    all_user_sentences.rename(columns={"id": "sentence_id"}, inplace=True)
    not_succeded_sentences = dfs["not_succesed_attempts"].copy()
    not_succeded_sentences.rename(columns={
        "attempt":"attempt_not_succeded"
    }, inplace=True)

    successful_user_translation = dfs["success"].copy()
    
    time_costs_for_user = dfs["progress"]
    still_mistakes = dfs["mistakes"].copy()

    # üîπ Rename columns for joining (—Ç–∞–∫ –∫–∞–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ detailed_mistakes –ü–æ–¥ sentence_id –ù–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è id_for_mistake_table)
    still_mistakes.rename(columns={
        "sentence_id": "id_for_mistake_table",
        "score": "current_score"
    }, inplace=True)

    successful_user_translation.rename(columns={
        "sentence_id": "id_for_mistake_table"
    }, inplace=True)   

    
    # üîπ Align date formats for join (set time to 00:00:00)
    #still_mistakes["added_data"] = pd.to_datetime(still_mistakes["added_data"]).dt.floor("D")
    all_user_sentences["date"] = pd.to_datetime(all_user_sentences["date"]).dt.floor("D")
    

    successful_user_translation.rename(columns={
        "date": "date_of_success"
    }, inplace=True)
    successful_user_translation["date_of_success"] = pd.to_datetime(successful_user_translation["date_of_success"].dt.floor("D"))
    
    # üîπ Merge base user sentence data
    ds_for_plot = all_user_sentences.merge(
        successful_user_translation,
        on="id_for_mistake_table", 
        how="left"
    )

    ds_for_plot = ds_for_plot.merge(
        time_costs_for_user[["session_id", "username", "start_time", "end_time"]],
        on="session_id", 
        how="left"
        )

    # üîπ Rename, compute durations
    ds_for_plot["start_session"] = ds_for_plot["start_time"]
    ds_for_plot["end_session"] = ds_for_plot["end_time"]
    ds_for_plot.drop(["start_time", "end_time"], axis="columns", inplace=True)

    ds_for_plot["session_duration"] = (
        ds_for_plot["end_session"] - ds_for_plot["start_session"]
    ).dt.total_seconds()

    # groupby("date"): –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –ø–æ –¥–∞—Ç–µ
    # ["sentence_id"]: –≤—ã–±–∏—Ä–∞–µ—Ç –∫–æ–ª–æ–Ω–∫—É sentence_id –≤ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø–µ
    # .count(): —Å—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–ø—É—Å—Ç—ã—Ö sentence_id –≤ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø–µ
    # transform("count"): –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–æ–ª—å–∫–æ –∂–µ —Å—Ç—Ä–æ–∫, —Å–∫–æ–ª—å–∫–æ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º DataFrame, —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–æ–∫ —Ç–æ–π –∂–µ –≥—Ä—É–ø–ø—ã
    # count() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –Ω–∞ –≥—Ä—É–ø–ø—É (–Ω–∞–ø—Ä–∏–º–µ—Ä: date | count)
    # but transform("count") –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –Ω–∞ –∫–∞–∂–¥—É—é –∑–∞–ø–∏—Å—å, –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ DataFrame ‚Äî –Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –¥–µ–ª–µ–Ω–∏—è
    ds_for_plot["sentences_in_session"] = ds_for_plot.groupby(["date", "session_id"])["sentence_id"].transform("count")

    ds_for_plot["avg_min_sentence_session"] = round(
        ((ds_for_plot["session_duration"] / ds_for_plot["sentences_in_session"]) / 60), 2
    )

    # üîπ Rename column for clarity
    ds_for_plot.rename(
        columns={
            "attempt": "attempt_successed",
            "score": "score_successed"}, 
        inplace=True
        )

    ds_for_plot["sentences_in_day"] = ds_for_plot.groupby("date")["sentence_id"].transform("count")
    session_durations = ds_for_plot.drop_duplicates("session_id")[["date", "session_id", "session_duration"]]
    total_time_per_day = session_durations.groupby("date")["session_duration"].sum()
    
    ds_for_plot = ds_for_plot.merge(
    total_time_per_day.rename("spent_time_per_day"),
    on="date",
    how="left"
    )

    ds_for_plot["avg_min_sentence_day"] = round(
        ((ds_for_plot["spent_time_per_day"] / ds_for_plot["sentences_in_day"]) / 60), 2
    )

    ds_for_plot = ds_for_plot.merge(
    not_succeded_sentences,
    on=["id_for_mistake_table", "user_id"],
    how="left"
    )

    # 5. –ò—Å–ø—Ä–∞–≤–∏—Ç—å –∫–æ–ª–æ–Ω–∫—É ds_for_plot["has_mistake"] = ds_for_plot["mistake_count"].notna() –¢–∞–∫ –∫–∞–∫ —É –Ω–∞—Å –±–æ–ª—å—à–µ –Ω–µ –±—É–¥–µ—Ç –ö–æ–ª–æ–Ω–∫–∏ mistake_count


    # üîπ Merge with mistake statistics
    ds_for_plot = ds_for_plot.merge(
        still_mistakes,
        on=["id_for_mistake_table"],
        how="left"
    )

    # üîπ Optional: add flag whether the sentence had a mistake
    ds_for_plot["user_stil_has_mistake"] = ds_for_plot["attempt_not_succeded"].notna()


    # üîπ Export to Excel (optional)
    ds_for_plot.to_excel("test_ds.xlsx", index=False)

    return ds_for_plot


if __name__ == "__main__":
    result = asyncio.run(prepare_aggregate_data_by_period_and_draw_analytic_for_user())

